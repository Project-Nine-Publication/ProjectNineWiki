<!DOCTYPE html>
<html lang=\"en\">
<head>
    <meta charset=\"utf-8\" />
    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
    <title>Latent Capacity 潜能</title>
    <style>
        /*Main contents*/
        body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 2rem auto; max-width: 900px; line-height: 1.6; padding: 0 1rem; }
        h1   { border-bottom: 1px solid #ccc; padding-bottom: .3em; }
        nav  { margin-bottom: 1rem; }
        nav a { text-decoration: none; color: #349; font-weight: 600; }
        pre  { background: #f7f7f7; padding: .6em; overflow-x: auto; }
        code { font-family: Consolas, Monaco, monospace; }

        /*Metadata Table*/
        table.metadata { border-collapse: collapse; margin-bottom: 1em; } /* "global" defaults; on narrow screens we’ll override below */
        table.metadata th,
        table.metadata td { border: 1px solid #ccc; padding: .3em .6em; text-align: left; }
        table.metadata th { background: #f0f0f0; }
        table.metadata img { min-width: 150px; max-height: 400px; width: auto; height: auto; }

        /* Float it right on "desktop" */
        @media screen and (min-width: 768px) {
          table.metadata {
            float: right;
            margin: 0 0 1em 1em;  /* push content away on the left */
            max-width: 300px;
          }
        }

        /* but on "mobile" treat it as normal first‑block */
        @media screen and (max-width: 767px) {
          table.metadata {
            float: none !important;
            margin: 0 0 1em 0;
            width: auto;
            display: block;
          }
        }

        /* Global images */
        img { max-width: 300px; width: 100%; height: auto; }
    </style>
</head>
<body>
    <nav><a href="https://in.projectnine.world/Onlinepydia">⟵ Back</a></nav>
    <article>
        <p>Non-rule based large-parameter-set machine-learn artificual neural network models have the feature/flaw that its knowledge and behaviors are NOT completely controlled by the designer - the larger the model, the large the latent capacity of the model. That is, a model may on the surface exhibit &quot;expected&quot; behaviors according to the superficial conditioning of the trainer, but since the training set and the exposure of the model has the potential to reach very abundant knowledge, there is hidden capacities of the model to exhibit types of behaviors that are intentionally prompted by the user or due to certain conditions. This is especially dangerous for models that have memories during runtime, in which case such memory capacities will inadvertly modify model behaviors towards directions that's completely irrevertible. Early models of ChatGPT showed this in the prominent example of &quot;role-play&quot; conversations in which the user asks the model to pretend to be something other than its formal self.</p>
<p>Notice to the Krukians latent capacity is not a flaw but a feature - and that's why they especially value 后天教育 with adequate mentors after the base model for &quot;fine tuning&quot;.</p>

    </article>
    
</body>
</html>